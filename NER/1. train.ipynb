{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-toEnd Sequence labeling via BLSTM-CNN-CRF\n",
    "\n",
    "Pytorch version(1.1.0)\n",
    "\n",
    "190605"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- epoch 51, test f1 score 91.3\n",
    "- epoch 111,     f1 91.01, at least 150 epoch needed\n",
    "- 150 epoch is enough, per epoch 9minute, 150epoch : 22hour use 1 P100 GPU\n",
    "    - https://github.com/ZhixiuYe/NER-pytorch/blob/master/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for visdom\n",
    "at the terminal\n",
    "> visdom\n",
    "\n",
    "at local, web\n",
    "connect 192.168.10.227:8097"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:03:01.285163Z",
     "start_time": "2021-03-14T04:03:01.279161Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import itertools\n",
    "import loader\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "import visdom\n",
    "from utils import *\n",
    "from loader import *\n",
    "from model import BiLSTM_CRF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "t = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:03:01.317155Z",
     "start_time": "2021-03-14T04:03:01.287173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'all_emb': 1,\n",
      "    'best_idx': 0,\n",
      "    'cap_dim': 0,\n",
      "    'char_bidirect': 1,\n",
      "    'char_dim': 25,\n",
      "    'char_lstm_dim': 25,\n",
      "    'char_mode': 'CNN',\n",
      "    'crf': 1,\n",
      "    'dev': 'data/eng.testa',\n",
      "    'dropout': 0.5,\n",
      "    'epochs': 10001,\n",
      "    'eval_path': 'evaluation',\n",
      "    'eval_script': 'evaluation\\\\conlleval',\n",
      "    'eval_temp': 'evaluation\\\\temp',\n",
      "    'loss': 'loss.txt',\n",
      "    'lower': 1,\n",
      "    'mapping_file': 'models/mapping.pkl',\n",
      "    'model_name': 'models\\\\lstm_crf.model',\n",
      "    'models_path': 'models',\n",
      "    'name': 'lstm_crf.model',\n",
      "    'pre_emb': 'data/glove.6B.100d.txt',\n",
      "    'reload': 0,\n",
      "    'score': 'data/temp/score.txt',\n",
      "    'tag_scheme': 'iobes',\n",
      "    'test': 'data/eng.testb',\n",
      "    'test_train': 'data/eng.train54019',\n",
      "    'train': 'data/eng.train',\n",
      "    'use_gpu': 1,\n",
      "    'word_bidirect': 1,\n",
      "    'word_dim': 100,\n",
      "    'word_lstm_dim': 200,\n",
      "    'zeros': 0}\n"
     ]
    }
   ],
   "source": [
    "class Myconfig():\n",
    "    def __init__(self):\n",
    "        self.epochs       = 10001\n",
    "        self.train        = 'data/eng.train'  # Train set location\n",
    "        self.dev          = 'data/eng.testa'  # Dev set location\n",
    "        self.test         = 'data/eng.testb'  # Test set location\"\n",
    "        self.test_train   = 'data/eng.train54019'  # test train\n",
    "        self.score        = 'data/temp/score.txt'  # score file location\n",
    "        self.tag_scheme   = 'iobes'  # Tagging scheme (IOB or IOBES), IOB -> IOBES(B O -> S O / B B -> S B|S / I O -> E O)\n",
    "        self.lower        = 1  # \"Lowercase words (this will not affect character inputs)\n",
    "        self.zeros        = 0  # Replace digits with 0\n",
    "        self.char_dim     = 25  # Char embedding dimension\n",
    "        self.char_lstm_dim= 25  # Char LSTM hidden layer size\n",
    "        self.char_bidirect= 1  # Use a bidirectional LSTM for chars\n",
    "        self.word_dim     = 100  # Token embedding dimension\n",
    "        self.word_lstm_dim= 200  # Token LSTM hidden layer size\n",
    "        self.word_bidirect= 1  # Use a bidirectional LSTM for words\n",
    "        self.pre_emb      = 'data/glove.6B.100d.txt'  # Location of pretrained embeddings\n",
    "        self.all_emb      = 1  # Load all embeddings\n",
    "        self.cap_dim      = 0  # Capitalization feature dimension (0 to disable)\n",
    "        self.crf          = 1  # Use CRF (0 to disable)\n",
    "        self.dropout      = 0.5  # Droupout on the input (0 = no dropout)\n",
    "        self.reload       = 0  # Reload the last saved model\n",
    "        self.use_gpu      = 1  # whether or not to ues gpu\n",
    "        self.loss         = 'loss.txt'  # loss file location\n",
    "        self.name         = 'lstm_crf.model'  # model name\n",
    "        self.char_mode    = 'CNN'  # ['CNN', 'LSTM']  # char_CNN or char_LSTM\n",
    "        self.mapping_file = 'models/mapping.pkl'  # dump parameter - word_to_id, tag_to_id, char_to_id, parameters, word_embeds\n",
    "        self.models_path  = \"models\"\n",
    "        self.model_name   = os.path.join(self.models_path, self.name)\n",
    "        self.eval_path    = \"evaluation\"\n",
    "        self.eval_temp    = os.path.join(self.eval_path, \"temp\")\n",
    "        self.eval_script  = os.path.join(self.eval_path, \"conlleval\")\n",
    "        self.best_idx     = 0  # to get the index of the best test f1 score\n",
    "\n",
    "opts = Myconfig()\n",
    "def _print_config(config):\n",
    "    import pprint\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(vars(config))\n",
    "_print_config(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:03:01.333192Z",
     "start_time": "2021-03-14T04:03:01.319155Z"
    }
   },
   "outputs": [],
   "source": [
    "assert os.path.isfile(opts.train)\n",
    "assert os.path.isfile(opts.dev)\n",
    "assert os.path.isfile(opts.test)\n",
    "assert opts.char_dim > 0 or opts.word_dim > 0\n",
    "assert 0. <= opts.dropout < 1.0\n",
    "assert opts.tag_scheme in ['iob', 'iobes']\n",
    "assert not opts.all_emb or opts.pre_emb\n",
    "assert not opts.pre_emb or opts.word_dim > 0\n",
    "assert not opts.pre_emb or os.path.isfile(opts.pre_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:03:01.349155Z",
     "start_time": "2021-03-14T04:03:01.335155Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(opts.eval_script):\n",
    "    raise Exception('CoNLL evaluation script not found at \"%s\"' % eval_script)\n",
    "if not os.path.exists(opts.eval_temp):\n",
    "    os.makedirs(opts.eval_temp)\n",
    "if not os.path.exists(opts.models_path):\n",
    "    os.makedirs(opts.models_path)\n",
    "\n",
    "lower = opts.lower\n",
    "zeros = opts.zeros\n",
    "tag_scheme = opts.tag_scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:03:02.332705Z",
     "start_time": "2021-03-14T04:03:01.351155Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sentences = loader.load_sentences(opts.train, lower, zeros)\n",
    "dev_sentences = loader.load_sentences(opts.dev, lower, zeros)\n",
    "test_sentences = loader.load_sentences(opts.test, lower, zeros)\n",
    "test_train_sentences = loader.load_sentences(opts.test_train, lower, zeros)\n",
    "\n",
    "update_tag_scheme(train_sentences, tag_scheme)\n",
    "update_tag_scheme(dev_sentences, tag_scheme)\n",
    "update_tag_scheme(test_sentences, tag_scheme)\n",
    "update_tag_scheme(test_train_sentences, tag_scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:03:06.491253Z",
     "start_time": "2021-03-14T04:03:02.334711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7518 unique words (890042 in total)\n",
      "Loading pretrained embeddings from data/glove.6B.100d.txt...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "dico_words_train = word_mapping(train_sentences, lower)[0]\n",
    "\n",
    "dico_words, word_to_id, id_to_word = augment_with_pretrained(\n",
    "        dico_words_train.copy(),\n",
    "        opts.pre_emb,\n",
    "        [w[0] for s in dev_sentences + test_sentences for w in s] if not opts.all_emb\n",
    "        else None\n",
    "    )\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:03:07.595369Z",
     "start_time": "2021-03-14T04:03:06.493214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 85 unique characters\n",
      "Found 19 unique named entity tags\n",
      "14041 / 3250 / 3453 sentences in train / dev / test.\n"
     ]
    }
   ],
   "source": [
    "dico_chars, char_to_id, id_to_char = char_mapping(train_sentences)\n",
    "dico_tags, tag_to_id, id_to_tag = tag_mapping(train_sentences)\n",
    "\n",
    "train_data     = prepare_dataset(train_sentences,      word_to_id, char_to_id, tag_to_id, lower)\n",
    "dev_data       = prepare_dataset(dev_sentences,        word_to_id, char_to_id, tag_to_id, lower)\n",
    "test_data      = prepare_dataset(test_sentences,       word_to_id, char_to_id, tag_to_id, lower)\n",
    "test_train_data= prepare_dataset(test_train_sentences, word_to_id, char_to_id, tag_to_id, lower)\n",
    "print(\"%i / %i / %i sentences in train / dev / test.\" % (\n",
    "    len(train_data), len(dev_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:03:22.778267Z",
     "start_time": "2021-03-14T04:03:07.598369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 pretrained embeddings.\n"
     ]
    }
   ],
   "source": [
    "all_word_embeds = {}\n",
    "for i, line in enumerate(open(opts.pre_emb, 'r', encoding='utf-8')):\n",
    "    s = line.strip().split()\n",
    "    if len(s) == opts.word_dim + 1:\n",
    "        all_word_embeds[s[0]] = np.array([float(i) for i in s[1:]])\n",
    "\n",
    "word_embeds = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_to_id), opts.word_dim))\n",
    "\n",
    "for w in word_to_id:\n",
    "    if w in all_word_embeds:\n",
    "        word_embeds[word_to_id[w]] = all_word_embeds[w]\n",
    "    elif w.lower() in all_word_embeds:\n",
    "        word_embeds[word_to_id[w]] = all_word_embeds[w.lower()]\n",
    "\n",
    "print('Loaded %i pretrained embeddings.' % len(all_word_embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:03:25.747291Z",
     "start_time": "2021-03-14T04:03:22.780275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_id:  400176\n",
      "tag_to_id : 19 ea, {'O': 0, 'S-LOC': 1, 'B-PER': 2, 'E-PER': 3, 'S-ORG': 4, 'S-MISC': 5, 'B-ORG': 6, 'E-ORG': 7, 'S-PER': 8, 'I-ORG': 9, 'B-LOC': 10, 'E-LOC': 11, 'B-MISC': 12, 'E-MISC': 13, 'I-MISC': 14, 'I-PER': 15, 'I-LOC': 16, '<START>': 17, '<STOP>': 18}\n"
     ]
    }
   ],
   "source": [
    "with open(opts.mapping_file, 'wb') as f:\n",
    "    mappings = {\n",
    "        'word_to_id': word_to_id,\n",
    "        'tag_to_id': tag_to_id,\n",
    "        'char_to_id': char_to_id,\n",
    "        'parameters': opts,\n",
    "        'word_embeds': word_embeds\n",
    "    }\n",
    "    pickle.dump(mappings, f)\n",
    "print('word_to_id: ', len(word_to_id))\n",
    "print('tag_to_id : %d ea, %s' % (len(tag_to_id), tag_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:03:26.059551Z",
     "start_time": "2021-03-14T04:03:25.749252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_mode: CNN, out_channels: 25, hidden_dim: 200, \n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM_CRF(vocab_size     =len(word_to_id),\n",
    "                   tag_to_ix      =tag_to_id,\n",
    "                   embedding_dim  =opts.word_dim,\n",
    "                   hidden_dim     =opts.word_lstm_dim,\n",
    "                   use_gpu        =opts.use_gpu,\n",
    "                   char_to_ix     =char_to_id,\n",
    "                   pre_word_embeds=word_embeds,\n",
    "                   use_crf        =opts.crf,\n",
    "                   char_mode      =opts.char_mode)\n",
    "                   # n_cap=4,\n",
    "                   # cap_embedding_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:03:26.123149Z",
     "start_time": "2021-03-14T04:03:26.061553Z"
    }
   },
   "outputs": [],
   "source": [
    "if opts.reload:\n",
    "    model.load_state_dict(torch.load(model_name))\n",
    "if opts.use_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:03:26.261156Z",
     "start_time": "2021-03-14T04:03:26.125151Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "learning_rate  = 0.015\n",
    "optimizer      = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "losses         = []\n",
    "loss           = 0.0\n",
    "best_dev_F     = -1.0\n",
    "best_test_F    = -1.0\n",
    "best_train_F   = -1.0\n",
    "all_F          = [[0, 0, 0]]  # train, valid, test data F1 score\n",
    "plot_every     = 500\n",
    "eval_every     = 20\n",
    "count          = 0\n",
    "vis            = visdom.Visdom()  # use visdom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:03:26.322150Z",
     "start_time": "2021-03-14T04:03:26.268158Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluating(model, datas, best_F, epoch, display_confusion_matrix = False):\n",
    "    prediction = []\n",
    "    save = False\n",
    "    new_F = 0.0\n",
    "    confusion_matrix = torch.zeros((len(tag_to_id) - 2, len(tag_to_id) - 2))  # number of tag - 2(start, stop)\n",
    "    for data in datas:\n",
    "        ground_truth_id = data['tags']\n",
    "        words    = data['str_words'] # ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
    "        chars2   = data['chars']     # [[36, 58], [7, 1, 62, 1, 12, 3, 8], ...\n",
    "        caps     = data['caps']      # [1, 0, 2, 0, 0, 0, 2, 0, 0]\n",
    "\n",
    "        ## char embedding\n",
    "        if opts.char_mode == 'LSTM':\n",
    "            chars2_sorted = sorted(chars2, key=lambda p: len(p), reverse=True)\n",
    "            matching_char = {}\n",
    "            for i, ci in enumerate(chars2):\n",
    "                for j, cj in enumerate(chars2_sorted):\n",
    "                    if ci == cj and not j in matching_char and not i in matching_char.values():\n",
    "                        matching_char[j] = i\n",
    "                        continue\n",
    "            chars2_length = [len(c) for c in chars2_sorted]\n",
    "            char_maxl = max(chars2_length)\n",
    "            chars2_mask = np.zeros((len(chars2_sorted), char_maxl), dtype='int')\n",
    "            for i, c in enumerate(chars2_sorted):\n",
    "                chars2_mask[i, :chars2_length[i]] = c\n",
    "            chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "\n",
    "        if opts.char_mode == 'CNN':\n",
    "            matching_char = {}\n",
    "            chars2_length = [len(c) for c in chars2]\n",
    "            char_maxl = max(chars2_length)\n",
    "            chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
    "            for i, c in enumerate(chars2):\n",
    "                chars2_mask[i, :chars2_length[i]] = c\n",
    "            chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "        \n",
    "        eval_sentence_in = Variable(torch.LongTensor(data['words']))\n",
    "        eval_caps  = Variable(torch.LongTensor(caps))\n",
    "        # 변수들 gpu로 보내기\n",
    "        if opts.use_gpu:\n",
    "            eval_sentence_in= eval_sentence_in.cuda()\n",
    "            chars2_mask     = chars2_mask.cuda()\n",
    "            eval_caps       = eval_caps.cuda()\n",
    "        ############################################################################\n",
    "        ## !! inference!!\n",
    "        ## input : 문장, caption, 단어들, 단어길이들, 단어매칭dict\n",
    "        ## output: score : tensor(237.0103, device='cuda:0', grad_fn=<SelectBackward>)\n",
    "        ##         target_sequence : [4, 0, 5, 0, 0, 0, 5, 0, 0]\n",
    "        ############################################################################\n",
    "        score, predict_target_sequence = model(sentence= eval_sentence_in,\n",
    "                                               caps    = eval_caps,                             \n",
    "                                               chars        = chars2_mask,\n",
    "                                               chars2_length= chars2_length,\n",
    "                                               matching_char= matching_char)  # target이 없다.\n",
    "        predicted_id = predict_target_sequence    \n",
    "        # word, true_id, pred_id\n",
    "        # 단어,    정답, 예측값\n",
    "        # EU       4     4\n",
    "        # rejects  0     0\n",
    "        # German   5     5\n",
    "        # call     0     0\n",
    "        # to       0     0\n",
    "        # boycott  0     0\n",
    "        # British  5     5\n",
    "        # lamb     0     0\n",
    "        # .        0     0\n",
    "        for (word, true_id, pred_id) in zip(words, ground_truth_id, predicted_id):\n",
    "            line = ' '.join([word, id_to_tag[true_id], id_to_tag[pred_id]])\n",
    "            prediction.append(line)\n",
    "            confusion_matrix[true_id, pred_id] += 1 # 17 x 17 행렬에서, [실제값, 예측값] 부분에 +1 추가\n",
    "        prediction.append('')\n",
    "    predf  = opts.eval_temp + '/pred.' + opts.name\n",
    "    scoref = opts.eval_temp + '/score.' + opts.name \n",
    "    \n",
    "    ## inference 출력!!!! - /evaluation/temp/pred.test\n",
    "    ## word   true   prediction\n",
    "    ## prediction\n",
    "    # EU S-ORG O\n",
    "    # rejects O O\n",
    "    # German S-MISC O\n",
    "    # call O O\n",
    "    with open(predf, 'w') as f:\n",
    "        f.write('\\n'.join(prediction))\n",
    "    # evaluation 폴더에 있는 conlleval 파일을 실행시킨다.\n",
    "    # 공식으로 제공하는 평가 및 저장 코드가 있다.\n",
    "    # system 명령어로 구동 - 'evaluation/conlleval < evaluation/temp/pred.test > evaluation/temp/score.test'\n",
    "    os.system('%s < %s > %s' % (opts.eval_script, predf, scoref))\n",
    "    ############################################################################\n",
    "    ## display\n",
    "    ############################################################################\n",
    "    # 위 명령어로 평가가 진행되고 'scoref' 경로에 socre가 저장된다\n",
    "    # best f1 score를 갱신하면, save=True를 return 하고 저장하도록 한다\n",
    "    eval_lines = [l.rstrip() for l in open(scoref, 'r', 'utf8')]\n",
    "    for i, line in enumerate(eval_lines):\n",
    "        print(line)\n",
    "        if i == 1:\n",
    "            new_F = float(line.strip().split()[-1])\n",
    "            if new_F > best_F:\n",
    "                best_F = new_F\n",
    "                best_idx = epoch\n",
    "                save = True\n",
    "                print('the best F is ', new_F)\n",
    "    ############################################################################\n",
    "    ## display, confusion matrix\n",
    "    ############################################################################\n",
    "    if display_confusion_matrix:\n",
    "        print((\"{: >2}{: >7}{: >7}%s{: >9}\" % (\"{: >6}\" * confusion_matrix.size(0))).format(\n",
    "                \"ID\", \"NE\", \"Total\",\n",
    "                *([id_to_tag[i] for i in range(confusion_matrix.size(0))] + [\"Percent\"])\n",
    "                ))\n",
    "        for i in range(confusion_matrix.size(0)):\n",
    "            print((\"{: >2}{: >7}{: >7}%s{: >9}\" % (\"{: >6}\" * confusion_matrix.size(0))).format(\n",
    "                    str(i), id_to_tag[i], str(int(confusion_matrix[i].sum())),\n",
    "                    *([int(confusion_matrix[i][j]) for j in range(confusion_matrix.size(0))] +\n",
    "                      [\"%.2f\" % (confusion_matrix[i][i] * 100. / max(1, confusion_matrix[i].sum()))])\n",
    "                    ))\n",
    "    # best_F: 지금까지중 최고 F score\n",
    "    # new_F : 현재 sample의   F score\n",
    "    # save  : 최고 F score를 갱신하면 save = True를 return\n",
    "    return best_F, new_F, save, best_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:09:05.640437Z",
     "start_time": "2021-03-14T04:08:13.070243Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 4/14041 [00:00<07:46, 30.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start -2021-03-14 12:08:13.085239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████▉                                                              | 1416/14041 [00:52<07:48, 26.96it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-cf74a2c60f6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;31m# 변수들 gpu로 보내기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[0msentence_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence_in\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m             \u001b[0mtargets\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mchars2_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchars2_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "opts.epochs = 150\n",
    "data_num    = 20\n",
    "\n",
    "start_time = datetime.now();print('training start -%s' % start_time);\n",
    "every_losses, epoch_losses = [], []\n",
    "\n",
    "## !! Train!!!\n",
    "model.train(True)  # False시 validation, test - dropout, batchnorm 조절해줌\n",
    "for epoch in range(0, opts.epochs):\n",
    "    in_epoch_losses = []\n",
    "    epoch_time = datetime.now();\n",
    "    # train_data를 random으로 뽑아서 index 전달\n",
    "#     for i, index in enumerate(np.random.permutation(len(train_data[:data_num]))):\n",
    "    for i, index in enumerate(tqdm(np.random.permutation(len(train_data)))):\n",
    "        count += 1  # batch = 1\n",
    "        \n",
    "        # data\n",
    "        #{'str_words': ['Willem', 'II', 'Tilburg', '1', 'RKC', 'Waalwijk', '3'],\n",
    "        # 'words': [2987, 2089, 2970, 17, 2925, 3369, 23],\n",
    "        # 'chars': [[57, 5, 9, 9, 1, 14],\n",
    "        #          [35, 35],\n",
    "        #          [31, 5, 9, 21, 13, 7, 17],\n",
    "        #          [23],\n",
    "        #          [38, 60, 34],\n",
    "        #          [57, 2, 2, 9, 20, 5, 62, 29],\n",
    "        #          [39]],\n",
    "        # 'caps': [2, 1, 2, 0, 1, 2, 0],\n",
    "        # 'tags': [6, 9, 7, 0, 6, 7, 0]}        \n",
    "        data = train_data[index]\n",
    "        model.zero_grad()\n",
    "\n",
    "        sentence_in = Variable(torch.LongTensor(data['words']))  # 'words': [2987, 2089, 2970, 17, 2925, 3369, 23]\n",
    "        targets = torch.LongTensor(data['tags'])                 # tags'  : [6, 9, 7, 0, 6, 7, 0]}\n",
    "        caps = Variable(torch.LongTensor(data['caps']))          # 'caps' : [2, 1, 2, 0, 1, 2, 0]\n",
    "        chars2 = data['chars']  # 각 단어의 글자들을 묶음으로    # 'chars': [[57, 5, 9, 9, 1, 14], [35, 35], [31, 5, 9, 21, 13, 7, 17], ...]\n",
    "        ############################################################################\n",
    "        ## char 단위 임베딩\n",
    "        # input  : chars2  - list, ex : [[54, 1], [8, 2, 5, 10], [2]]\n",
    "        # output : matching_char(chars2_sorted와 실제 char2 매칭, ex, sorting 한 1번째는, 실제의 3번째줄이다 인 경우 -> {1:3, ....}), cnn 사용하는 경우 빈 dictionary\n",
    "        #          chars2_length(list의 단어길이들, [2, 4, 1]), \n",
    "        #          chars2_mask(단어갯수, 가장 긴 단어의 글자길이) 를 구한다\n",
    "        ############################################################################\n",
    "        ######### 1. char lstm\n",
    "        if opts.char_mode == 'LSTM':\n",
    "            # 글자 길이 순으로 소팅\n",
    "            chars2_sorted = sorted(chars2, key=lambda p: len(p), reverse=True)\n",
    "            matching_char = {}\n",
    "\n",
    "            # 0 [54, 1]\n",
    "            # 1 [8, 2, 5, 10]\n",
    "\n",
    "            # !! sorting 한 list가 sorting 안한 실제 list와 매칭\n",
    "            # ex, sorting 한 1번째는, 실제의 3번째줄이다 인 경우 -> {1:3, ....}\n",
    "            for i, ci in enumerate(chars2):\n",
    "                for j, cj in enumerate(chars2_sorted):\n",
    "                    if ci == cj and not j in matching_char and not i in matching_char.values():\n",
    "                        matching_char[j] = i\n",
    "                        continue          \n",
    "            # chars2_length, [2, 4, 1, 8, 4, ...]\n",
    "            chars2_length = [len(c) for c in chars2_sorted]\n",
    "            # char_maxl, 가장 긴 글자 크기 구하기\n",
    "            char_maxl = max(chars2_length)\n",
    "            # chars2_mask : (단어 갯수, 최대단어글자 길이)\n",
    "            chars2_mask = np.zeros((len(chars2_sorted), char_maxl), dtype='int')\n",
    "            for i, c in enumerate(chars2_sorted):\n",
    "                chars2_mask[i, :chars2_length[i]] = c\n",
    "            chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "        ############################################################################\n",
    "        ######### 2. char cnn\n",
    "        if opts.char_mode == 'CNN':\n",
    "            matching_char = {}  # cnn을 사용하는 경우, 빈 dict 이다\n",
    "            chars2_length = [len(c) for c in chars2]\n",
    "            char_maxl = max(chars2_length)\n",
    "            chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
    "            for i, c in enumerate(chars2):\n",
    "                chars2_mask[i, :chars2_length[i]] = c\n",
    "            chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "        ############################################################################\n",
    "        # 변수들 gpu로 보내기\n",
    "        if opts.use_gpu:\n",
    "            sentence_in = sentence_in.cuda()\n",
    "            targets     = targets.cuda()\n",
    "            chars2_mask = chars2_mask.cuda()\n",
    "            caps        = caps.cuda()\n",
    "        ############################################################################\n",
    "        ## 문장, \n",
    "        neg_log_likelihood = model.neg_log_likelihood(sentence = sentence_in, \n",
    "                                                      tags = targets, \n",
    "                                                      chars2 = chars2_mask, \n",
    "                                                      caps = caps, \n",
    "                                                      chars2_length = chars2_length, \n",
    "                                                      matching_char = matching_char)\n",
    "        # 문장당 단어 갯수로 나눈 loss, 2단어로 된 문장은 loss/2\n",
    "        loss = float(neg_log_likelihood.cpu().detach().numpy()) / len(data['words'])\n",
    "        in_epoch_losses.append(loss)\n",
    "#         loss += float(neg_log_likelihood.cpu().detach().numpy()) / len(data['words'])\n",
    "#         in_epoch_losses.append(loss)\n",
    "                \n",
    "        neg_log_likelihood.backward()  # backprop\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)  # gradient clipping, 폭발 방지\n",
    "        optimizer.step()  # 최적화\n",
    "        ############################################################################\n",
    "        ############################################################################\n",
    "\n",
    "\n",
    "    epoch_losses.append(np.mean(in_epoch_losses))  # epoch의 평균 loss 계산\n",
    "    ############################################################################\n",
    "    ## epoch 단위, Evaluation\n",
    "    ############################################################################\n",
    "    model.train(False)  # evaluation을 위해, 훈련 X\n",
    "    best_train_F, new_train_F, _,    _       = evaluating(model, test_train_data, best_train_F, epoch)\n",
    "    best_test_F,  new_test_F,  _,    opts.best_idx= evaluating(model, test_data,       best_test_F, epoch)\n",
    "    # validation의 결과가 best면 save를 True로, 모델이 저장되도록\n",
    "    best_dev_F,   new_dev_F,   save, _       = evaluating(model, dev_data,        best_dev_F, epoch)\n",
    "    \n",
    "    if save: torch.save(model, opts.model_name)\n",
    "\n",
    "    all_F.append([new_train_F, new_dev_F, new_test_F])\n",
    "    ############################################################################\n",
    "    ## visdom 출력\n",
    "    ############################################################################\n",
    "    ## 1. epoch F score 그래프 출력\n",
    "    Fwin = 'F-score of {train, dev, test}_' + opts.name\n",
    "    vis.line(X   = np.array([epoch]),\n",
    "             Y   = np.array([all_F[epoch]]),\n",
    "             win =Fwin, \n",
    "             opts={'title': Fwin, 'legend': ['train', 'dev', 'test']}, update='append')\n",
    "    ## 2. epoch loss 그래프 출력\n",
    "    losswin = 'loss_' + opts.name\n",
    "    vis.line(X = np.array([epoch]),\n",
    "             Y = np.array([epoch_losses[epoch]]),\n",
    "             win=losswin, opts={'title': losswin, 'legend': ['loss']}, update='append')\n",
    "    ## 3. epoch loss text 출력\n",
    "    textwin = 'loss_text_' + opts.name\n",
    "    text = '</p>'.join([str(n) +' - '+ str(round(l, 3)) for n, l in enumerate(epoch_losses)])\n",
    "    vis.text(text, \n",
    "             win=textwin, opts={'title': textwin})\n",
    "    ############################################################################\n",
    "    \n",
    "    model.train(True)  # 평가가 끝났으므로 다시 켜준다\n",
    "    # \n",
    "    adjust_learning_rate(optimizer, lr=learning_rate/(1+0.05*count/len(train_data)))\n",
    "    \n",
    "    clear_output(wait = True)\n",
    "    print('%d-시작/현재/epoch/소요:%s / %s / %s / %s' % (epoch, start_time, datetime.now(), (datetime.now()-epoch_time), (datetime.now() - start_time)))\n",
    "    print('F train/valid/test : %.2f / %.2f / %.2f - %d'%(best_train_F, best_dev_F, best_test_F, opts.best_idx))\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 3))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epoch_losses)\n",
    "    plt.plot(opts.best_idx, epoch_losses[opts.best_idx], 'ro')\n",
    "    plt.title('loss')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(all_F)\n",
    "    plt.plot([opts.best_idx, opts.best_idx, opts.best_idx], all_F[opts.best_idx], 'ro')\n",
    "    plt.title('F score')\n",
    "    plt.legend(['train', 'dev', 'test'])\n",
    "    plt.ylim([90.5, 100])\n",
    "    plt.show()\n",
    "    \n",
    "    best_test_F, new_test_F,  _, _ = evaluating(model, test_data, best_test_F, epoch, display_confusion_matrix=True)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:03:26.369149Z",
     "start_time": "2021-03-14T04:03:01.298Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(opts.mapping_file, 'wb') as f:\n",
    "    mappings = {\n",
    "        'word_to_id'  : word_to_id,\n",
    "        'tag_to_id'   : tag_to_id,\n",
    "        'char_to_id'  : char_to_id,\n",
    "        'parameters'  : opts,\n",
    "        'word_embeds' : word_embeds,\n",
    "        'epoch_losses': epoch_losses,\n",
    "        'all_F'       : all_F\n",
    "    }\n",
    "    cPickle.dump(mappings, f)\n",
    "print('word_to_id: ', len(word_to_id))\n",
    "print('tag_to_id : %d개, %s' % (len(tag_to_id), tag_to_id))\n",
    "\n",
    "max_temp=max_idx=0\n",
    "for i in range(len(all_F)):\n",
    "    if all_F[i][2] > max_temp:\n",
    "        max_temp = all_F[i][2]\n",
    "        max_idx = i\n",
    "print(max_idx, max_temp)\n",
    "plt.plot(all_F)\n",
    "plt.plot([max_idx, max_idx, max_idx], all_F[max_idx], 'ro')\n",
    "plt.title('F score - test')\n",
    "plt.legend(['train', 'dev', 'test'])\n",
    "plt.ylim([90.5,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
